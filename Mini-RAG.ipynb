{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a24b35-4892-409e-aef4-51b535526c4d",
   "metadata": {},
   "source": [
    "Check Kernel Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25383f01-393f-48e8-9283-226a9d506dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\aldha\\\\Projects\\\\mini-rag\\\\.venv\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaeeca0-39f5-4926-b67c-5a6e86de8b9e",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b297ed-a1a9-45ec-b75a-db054b89adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (0.11.8)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: google-genai in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (1.56.0)\n",
      "Requirement already satisfied: gradio in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (6.2.0)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from pdfplumber) (12.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from pdfplumber) (5.2.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from beautifulsoup4) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from google-genai) (4.12.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.45.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.45.0->google-genai) (2.45.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from google-genai) (2.12.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from google-genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from google-genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from google-genai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from google-genai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (6.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.6.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (0.6.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (0.2.2)\n",
      "Requirement already satisfied: brotli>=1.1.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (0.127.0)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.2 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (2.0.2)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (0.36.0)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (3.0.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (2.4.0)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (3.11.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (2.3.3)\n",
      "Requirement already satisfied: pydub in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (0.0.21)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (6.0.3)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (0.20.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio) (0.40.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from gradio-client==2.0.2->gradio) (2025.12.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber beautifulsoup4 nltk tqdm google-genai gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc7b358-38e0-4550-b06c-9197ca389f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aldha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\aldha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pdfplumber\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2327897-e405-4267-bb98-bf117fd3ae88",
   "metadata": {},
   "source": [
    "## 1. Prepare the Document Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7286ed-6fed-43e2-b780-b4cbb1aa93ab",
   "metadata": {},
   "source": [
    "Di processing step, dilakukan pengambilan data dari `corpus/raw` kemudian diambil text nya, dan dimasukkan ke `corpus/processed/docs` dengan sequence sebagai name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7ac3f-32c6-4be7-a756-c1da35a46e4a",
   "metadata": {},
   "source": [
    "### 1.1. Fungsi Clean Text (Remove duplicate space & normalize new lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe1a552-b481-468a-820b-198c9e3cc79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"\\s+\", \" \", text)        # remove extra spaces\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text)       # normalize newlines\n",
    "    text = re.sub(r\"\\t+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b1765-169f-4414-a3d8-154dbde158b8",
   "metadata": {},
   "source": [
    "### 1.2. Fungsi untuk extract text dari file .pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9814e7f3-2a32-4f7c-95aa-a5f1a9b66f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return clean_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817056bc-7e10-48d6-b1f3-f75267db9a4b",
   "metadata": {},
   "source": [
    "### 1.3. Fungsi untuk extract text dari file .html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b3fd8d-3fd7-47d2-bf8f-15a51edf5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html(html_path: str) -> str:\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    # remove script & style\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    return clean_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf08697-f430-4804-bff5-ec25516224c2",
   "metadata": {},
   "source": [
    "### 1.4. Fungsi untuk extract text dari file .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057a3520-8d6a-434e-ba32-483b24559eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_txt(txt_path: str) -> str:\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return clean_text(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991715f-b35d-48ce-953b-638b427994bd",
   "metadata": {},
   "source": [
    "### 1.5. Scan folder `corpus/raw` extract text ke `corpus/processed/docs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af22e4e-7ae3-447f-b322-6f71ad40d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 18558.87it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 35.69it/s]\n"
     ]
    }
   ],
   "source": [
    "RAW_DIR = \"corpus/raw\"\n",
    "OUT_DIR = \"corpus/processed/docs\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "doc_id = 1\n",
    "\n",
    "for root, _, files in os.walk(RAW_DIR):\n",
    "    for file in tqdm(files):\n",
    "        path = os.path.join(root, file)\n",
    "\n",
    "        try:\n",
    "            if file.endswith(\".html\"):\n",
    "                text = extract_text_from_html(path)\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if len(text) < 500:  # skip dokumen terlalu pendek\n",
    "                continue\n",
    "\n",
    "            out_path = os.path.join(OUT_DIR, f\"doc_{doc_id:02d}.txt\")\n",
    "            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "\n",
    "            doc_id += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c4910-0fd9-404a-8b9d-0c3c055db9fb",
   "metadata": {},
   "source": [
    "## 2. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1062b39-01dd-4c73-8717-a3b1fea1ede3",
   "metadata": {},
   "source": [
    "### 2.1. Load Docs yang ada di `corpus/processed/docs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d37dd6-5366-4bf8-a9db-f8fa1430de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22 documents\n"
     ]
    }
   ],
   "source": [
    "DOC_DIR = \"corpus/processed/docs\"\n",
    "docs = sorted(os.listdir(DOC_DIR))\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76068c4d-e121-4f3d-ad8a-3d3fcb4a330a",
   "metadata": {},
   "source": [
    "### 2.2. Fungsi Chunk text dengan tokenizer NLTK, dengan default Chunk Size = 150, dan Overlap = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acb3e423-b524-41c3-9117-1c6b3ca39ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_size: int = 150,\n",
    "    overlap: int = 30\n",
    "):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    chunks = []\n",
    "\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + chunk_size\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = \" \".join(chunk_tokens)\n",
    "\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "        start = end - overlap  # overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255314d3-3ebc-4cae-8c79-3fd0241a5bec",
   "metadata": {},
   "source": [
    "### 2.3. Iterate semua docs, dan chunk text kemudian dimasukan ke `corpus/chunks/chunks.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2808464e-677d-49db-b0da-1eb9dcb2fbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 177.79it/s]\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "chunk_id = 0\n",
    "\n",
    "for doc_id, file in enumerate(tqdm(docs)):\n",
    "    path = os.path.join(DOC_DIR, file)\n",
    "    \n",
    "    if not file.lower().endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    \n",
    "        chunks = chunk_text(text)\n",
    "    \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            all_chunks.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"doc_name\": file,\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk\n",
    "            })\n",
    "            chunk_id += 1\n",
    "            \n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"❌ Error encoding di file: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error lain pada {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41b17c7a-2a61-4eee-b91a-f41bc8d22a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 245 chunks\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "OUT_PATH = \"corpus/chunks/chunks.json\"\n",
    "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(all_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac26cf-5d7b-4e21-b8b6-b1aeedecd96e",
   "metadata": {},
   "source": [
    "### 2.4. Preview Chunking Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14d3378e-df33-4a46-b25d-bb3750d432c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 0 ---\n",
      "Doc: doc_01.txt\n",
      "15.687 Pengungsi Bencana di Sumut Terpapar Penyakit Kulit login register Nasional Peristiwa 15.687 Pengungsi Bencana di Sumut Terpapar Penyakit Kulit CNN Indonesia Selasa , 23 Des 2025 13:52 WIB Bagikan : url telah tercopy Dinas Kesehatan Provinsi Sumatera Utara mencatat ada 15.687 warga terdampak b ...\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Doc: doc_01.txt\n",
      "Kasus terbanyak ditemukan di Kabupaten Langkat , Tapanuli Tengah , Deliserdang , Batubara , Tebingtinggi , dan Mandailing Natal . `` Ini perlu menjadi fokus perhatian , terutama terkait faktor risiko seperti paparan air kotor , sanitasi lingkungan yang belum optimal , keterbatasan air bersih , serta ...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Doc: doc_01.txt\n",
      "banjir . `` Sekarang Tapteng mulai kering , sehingga keluhan ISPA meningkat , '' ujarnya . Pilihan Redaksi Pengungsi Banjir-Longsor Sumut Mulai Terserang Penyakit Kulit dan ISPA 100 Nakes Sulsel Dikirim ke Daerah Bencana Aceh Tamiang Gelombang II Bantuan Internasional ke Aceh : Obat-obatan hingga Re ...\n"
     ]
    }
   ],
   "source": [
    "for c in all_chunks[:3]:\n",
    "    print(\"\\n--- Chunk\", c[\"chunk_id\"], \"---\")\n",
    "    print(\"Doc:\", c[\"doc_name\"])\n",
    "    print(c[\"text\"][:300], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c46fa43-7ac0-4cdd-932c-2dd1a934c2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk stats:\n",
      "  Total chunks      : 245\n",
      "  Avg tokens/chunk  : 142\n",
      "  Min tokens/chunk  : 11\n",
      "  Max tokens/chunk  : 152\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(nltk.word_tokenize(c[\"text\"])) for c in all_chunks]\n",
    "\n",
    "print(\"Chunk stats:\")\n",
    "print(f\"  Total chunks      : {len(all_chunks)}\")\n",
    "print(f\"  Avg tokens/chunk  : {sum(lengths)//len(lengths)}\")\n",
    "print(f\"  Min tokens/chunk  : {min(lengths)}\")\n",
    "print(f\"  Max tokens/chunk  : {max(lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8395e3d-e3a8-4be2-9253-93f3a9bdadc5",
   "metadata": {},
   "source": [
    "## 3. Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60668cc6-66ec-4b50-9429-5d3ca819a5a2",
   "metadata": {},
   "source": [
    "Dalam process embedding, saya menggunakan `intfloat/multilingual-e5-base` dan `BAAI/bge-m3` sebagai perbandingan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf1289-5e00-4843-813d-686679574528",
   "metadata": {},
   "source": [
    "### 3.1. Load Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78c8ea8-c6ea-4afe-a035-9350471ac379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"corpus/chunks/chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "texts = [c[\"text\"] for c in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd699378-7076-499c-ac80-c1fc393dbbd3",
   "metadata": {},
   "source": [
    "### 3.2. Define Compared Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b9ce6a3-3951-41e0-9368-1477a28dd499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.12.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\aldha\\projects\\mini-rag\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84d493f2-4723-4e2b-a12e-514bde8dcf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aldha\\Projects\\mini-rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "embedding_models = {\n",
    "    \"bge_m3\": \"BAAI/bge-m3\",\n",
    "    \"e5_multilingual\": \"intfloat/multilingual-e5-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86860e5d-0e0e-4522-b635-ac792beddfb2",
   "metadata": {},
   "source": [
    "### 3.3. Encode semua texts yang di dapat dari chunks sesuai embedding_models, dan simpan hasil embedding docs di local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5af04a-5cc1-4f79-8de0-f90a6d97344f",
   "metadata": {},
   "source": [
    "Iterate semua models yang dipilih dan encode chunks text ke dalam embedding di local dengan nama embeddings_{key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27c0944f-4f69-48ae-a8e6-b5951b8e7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding with BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 8/8 [01:44<00:00, 13.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved corpus/chunks/embeddings_bge_m3.npy | shape = (245, 1024)\n",
      "Embedding with intfloat/multilingual-e5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████| 8/8 [00:30<00:00,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved corpus/chunks/embeddings_e5_multilingual.npy | shape = (245, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "os.makedirs(\"corpus/chunks\", exist_ok=True)\n",
    "\n",
    "for key, model_name in embedding_models.items():\n",
    "    print(f\"Embedding with {model_name}\")\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    out_path = f\"corpus/chunks/embeddings_{key}.npy\"\n",
    "    np.save(out_path, embeddings)\n",
    "\n",
    "    print(f\"Saved {out_path} | shape = {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef332be6-9bf0-45b1-a0d8-711ad7c87d77",
   "metadata": {},
   "source": [
    "## 4. Vector Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f3d7e-4d83-47a5-86a9-4f6e6c072e3d",
   "metadata": {},
   "source": [
    "### 4.1. Import & Load Data dari chunks, dan embedded chunks sesuai dengan embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9382d625-0eac-4111-8667-14a83cbbf4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93d706c2-6855-42e8-a413-a1c5be872bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 245\n",
      "Embeddings shape: (245, 1024)\n"
     ]
    }
   ],
   "source": [
    " # Load embeddings (pilih salah satu / bandingkan dua)\n",
    "embeddings_bge = np.load(\"corpus/chunks/embeddings_bge_m3.npy\")\n",
    "embeddings_e5  = np.load(\"corpus/chunks/embeddings_e5_multilingual.npy\")\n",
    "\n",
    "print(\"Chunks:\", len(chunks))\n",
    "print(\"Embeddings shape:\", embeddings_bge.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01171bbd-f073-46a8-b3e7-98df10fde95c",
   "metadata": {},
   "source": [
    "### 4.2.Ambil Embedding models, untuk encode query yang diberikan user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "805bb62c-4d3a-4822-b30b-fc29c87a0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "query_model_bge = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "query_model_e5  = SentenceTransformer(\"intfloat/multilingual-e5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047be14-412a-4bd9-89cb-9e2e546c558d",
   "metadata": {},
   "source": [
    "Fungsi untuk embedding_query sesuai model yang dipilih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3514e9d5-a1e8-4b32-b4ca-92aa5626401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_query(query: str, model: SentenceTransformer):\n",
    "    return model.encode(\n",
    "        query,\n",
    "        normalize_embeddings=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265eed9a-ea7c-4719-b3fe-bf28f46ab82a",
   "metadata": {},
   "source": [
    "### 4.3. Buat Fungsi MMR (Maximal Marginal Relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a22d59bb-9d6d-476d-b248-947bc7a806ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def mmr(\n",
    "    query_embedding,\n",
    "    doc_embeddings,\n",
    "    top_k=3,\n",
    "    lambda_param=0.7\n",
    "):\n",
    "    query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "    relevance = cosine_similarity(doc_embeddings, query_embedding).reshape(-1)\n",
    "\n",
    "    selected_idx = []\n",
    "    candidate_idx = list(range(len(doc_embeddings)))\n",
    "\n",
    "    for _ in range(top_k):\n",
    "        if len(selected_idx) == 0:\n",
    "            idx = np.argmax(relevance)\n",
    "            selected_idx.append(idx)\n",
    "            candidate_idx.remove(idx)\n",
    "            continue\n",
    "\n",
    "        mmr_scores = []\n",
    "\n",
    "        for i in candidate_idx:\n",
    "            redundancy = max(\n",
    "                cosine_similarity(\n",
    "                    doc_embeddings[i].reshape(1, -1),\n",
    "                    doc_embeddings[selected_idx]\n",
    "                )[0]\n",
    "            )\n",
    "\n",
    "            score = (\n",
    "                lambda_param * relevance[i]\n",
    "                - (1 - lambda_param) * redundancy\n",
    "            )\n",
    "            mmr_scores.append(score)\n",
    "\n",
    "        best = candidate_idx[np.argmax(mmr_scores)]\n",
    "        selected_idx.append(best)\n",
    "        candidate_idx.remove(best)\n",
    "\n",
    "    return selected_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ef1fd1-aa5e-4b53-808f-4b851826847c",
   "metadata": {},
   "source": [
    "### 4.4. Buat Fungsi Retrieve dengan apply MMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83aa7a64-ba59-48e2-a749-68164c4e4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_with_mmr(\n",
    "    query_embedding,\n",
    "    embeddings,\n",
    "    chunks,\n",
    "    k=3\n",
    "):\n",
    "    idxs = mmr(\n",
    "        query_embedding,\n",
    "        embeddings,\n",
    "        top_k=k\n",
    "    )\n",
    "\n",
    "    return [chunks[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682435b8-0209-4960-a293-fb9c63156406",
   "metadata": {},
   "source": [
    "### 4.5. Buat fungsi Build Context untuk membuat context dari chunks yang di ambil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b607aa3c-5de4-4927-ba9e-2bbbaf628091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(retrieved_chunks):\n",
    "    return \"\\n\\n\".join(\n",
    "        [f\"- {c['text']}\" for c in retrieved_chunks]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246b2a5-8140-44f4-b183-31db6676d535",
   "metadata": {},
   "source": [
    "## 5. RAG Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fece3c-04e9-4013-9dd0-2b4769ca07c5",
   "metadata": {},
   "source": [
    "### 5.1. Init API KEY (Menggunakan LLM Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d20d21c-22ca-4856-9b0a-a9dd5d049a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec0e11-8654-4228-b6d0-7e20fb0bdbfa",
   "metadata": {},
   "source": [
    "### 5.2. Init Google Generative AI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84a87f04-a1b7-47c9-8fd6-bcd1f5bee876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "ai_client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279d389-66b0-41b9-99f5-2e654ae545d3",
   "metadata": {},
   "source": [
    "### 5.3. Menggunakan Model Gemini 2.0 Flash, karena model Gemini 1.5 sudah tidak tersedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba4b04d1-af15-43c7-a2b2-bf79dd445a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FLASH = \"gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7876621d-9819-4600-9e4b-74a7fcbe8782",
   "metadata": {},
   "source": [
    "### 5.4. Membuat Fungsi untuk Membuat prompt dengan menambahkan context dari Retrieved Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8846cd6a-45fb-4237-b30e-c52c72eea369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_gemini(\n",
    "    client,\n",
    "    question: str,\n",
    "    context: str = None,\n",
    "    model=MODEL_FLASH\n",
    "):\n",
    "    if context:\n",
    "        prompt = f\"\"\"\n",
    "            You are an AI assistant. Answer the question using ONLY the context below.\n",
    "            If the answer is not in the context, say \"Information not found in the provided documents.\"\n",
    "            \n",
    "            Context:\n",
    "            {context}\n",
    "            \n",
    "            Question:\n",
    "            {question}\n",
    "            \n",
    "            Answer:\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "            Question:\n",
    "            {question}\n",
    "            \n",
    "            Answer concisely:\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa27fbd-986d-41bd-bbd1-d2c36ec8b2cc",
   "metadata": {},
   "source": [
    "### 5.5. Run LLM-RAG menggunakan gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e77e572-d200-40f0-9dc2-2ccf87a34ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        NON-RAG Answers:\n",
      "        ==================================================\n",
      "        answers=Ya, ada laporan tentang fenomena langit merah di Indonesia akhir-akhir ini, terutama dikaitkan dengan peningkatan polusi udara dan partikel debu di atmosfer.\n",
      "\n",
      "    \n",
      "\n",
      "        RAG Answers:\n",
      "        ==================================================\n",
      "    \n",
      "\n",
      "            k=1\n",
      "            embeddings=BAAI/bge-m3\n",
      "            answers=Ya, terjadi fenomena langit merah pada Kamis (18/12) petang di Pandeglang.\n",
      "\n",
      "        \n",
      "\n",
      "            k=1\n",
      "            embeddings=intfloat/multilingual-e5-base\n",
      "            answers=Ya, fenomena alam langit merah terjadi di wilayah Pandeglang selatan pada Kamis (18/12) petang.\n",
      "\n",
      "        \n",
      "\n",
      "            k=2\n",
      "            embeddings=BAAI/bge-m3\n",
      "            answers=Sebagai informasi , fenomena alam langit merah di wilayah Pandeglang selatan terjadi pada Kamis ( 18/12 ) petang .\n",
      "\n",
      "        \n",
      "\n",
      "            k=2\n",
      "            embeddings=intfloat/multilingual-e5-base\n",
      "            answers=Ya, sebagai informasi, fenomena alam langit merah terjadi di wilayah Pandeglang selatan pada Kamis (18/12) petang.\n",
      "\n",
      "        \n",
      "\n",
      "            k=3\n",
      "            embeddings=BAAI/bge-m3\n",
      "            answers=Ya, fenomena alam langit merah terjadi di wilayah Pandeglang selatan pada Kamis (18/12) petang.\n",
      "\n",
      "        \n",
      "\n",
      "            k=3\n",
      "            embeddings=intfloat/multilingual-e5-base\n",
      "            answers=Masyarakat Kabupaten Pandeglang , Banten , dibuat heboh dengan kemunculan langit berwarna merah gelap beberapa hari lalu.\n",
      "\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def answer_with_k(query, k):\n",
    "    answers_dict = []\n",
    "\n",
    "    query_bge = embed_query(query, query_model_bge)\n",
    "    query_e5 = embed_query(query, query_model_e5)\n",
    "    \n",
    "    # embeddings: bge-m3\n",
    "    retrieved = retrieve_with_mmr(\n",
    "        query_embedding=query_bge,\n",
    "        embeddings=embeddings_bge,\n",
    "        chunks=chunks,\n",
    "        k=k\n",
    "    )\n",
    "    context = build_context(retrieved)\n",
    "    answer = generate_answer_gemini(\n",
    "        client=ai_client,\n",
    "        question=query,\n",
    "        context=context,\n",
    "        model=MODEL_FLASH\n",
    "    )\n",
    "    answers_dict.append({\n",
    "        'k': k,\n",
    "        'embeddings': 'BAAI/bge-m3',\n",
    "        'answers': answer,\n",
    "    })\n",
    "\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # embeddings: e5\n",
    "    retrieved = retrieve_with_mmr(\n",
    "        query_embedding=query_e5,\n",
    "        embeddings=embeddings_e5,\n",
    "        chunks=chunks,\n",
    "        k=k\n",
    "    )\n",
    "    context = build_context(retrieved)\n",
    "    answer = generate_answer_gemini(\n",
    "        client=ai_client,\n",
    "        question=query,\n",
    "        context=context,\n",
    "        model=MODEL_FLASH\n",
    "    )\n",
    "    answers_dict.append({\n",
    "        'k': k,\n",
    "        'embeddings': 'intfloat/multilingual-e5-base',\n",
    "        'answers': answer,\n",
    "    })\n",
    "    return answers_dict\n",
    "\n",
    "\n",
    "def rag_chat(question):\n",
    "    answers = []\n",
    "    final_answers = \"\"\n",
    "\n",
    "     # Non RAG\n",
    "    answer_non_rag = generate_answer_gemini(\n",
    "        ai_client, \n",
    "        question, \n",
    "        None, \n",
    "        MODEL_FLASH\n",
    "    )\n",
    "\n",
    "    final_answers += f\"\"\"\n",
    "        NON-RAG Answers:\n",
    "        ==================================================\n",
    "        answers={answer_non_rag}\n",
    "    \"\"\"\n",
    "    \n",
    "    answers_k = answer_with_k(query=question, k=1)\n",
    "    answers.extend(answers_k)\n",
    "    \n",
    "    answers_k = answer_with_k(query=question, k=2)\n",
    "    answers.extend(answers_k)\n",
    "    \n",
    "    answers_k = answer_with_k(query=question, k=3)\n",
    "    answers.extend(answers_k)\n",
    "\n",
    "    final_answers += \"\"\"\\n\n",
    "        RAG Answers:\n",
    "        ==================================================\n",
    "    \"\"\"\n",
    "    for ans in answers:\n",
    "        final_answers += \"\\n\"\n",
    "        answers_text = f\"\"\"\n",
    "            k={ans['k']}\n",
    "            embeddings={ans['embeddings']}\n",
    "            answers={ans['answers']}\n",
    "        \"\"\"\n",
    "        final_answers += answers_text\n",
    "    \n",
    "    return final_answers\n",
    "\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=rag_chat,\n",
    "    inputs=gr.Textbox(label=\"Ask a question\"),\n",
    "    outputs=gr.Textbox(label=\"RAG Answer\"),\n",
    "    title=\"Mini RAG Demo\",\n",
    "    description=\"Simple Retrieval-Augmented Generation system\"\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef593a-083d-43a9-8ddb-7912246b3481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mini-rag)",
   "language": "python",
   "name": "mini-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
