{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcaeeca0-39f5-4926-b67c-5a6e86de8b9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55e7a351-4554-4df6-9e0a-9097ca291808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dhanarjkusuma/Projects/magister/mini-rag/.venv/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b297ed-a1a9-45ec-b75a-db054b89adc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in ./.venv/lib/python3.14/site-packages (0.11.8)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.venv/lib/python3.14/site-packages (4.14.3)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.14/site-packages (3.9.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.14/site-packages (4.67.1)\n",
      "Requirement already satisfied: pdfminer.six==20251107 in ./.venv/lib/python3.14/site-packages (from pdfplumber) (20251107)\n",
      "Requirement already satisfied: Pillow>=9.1 in ./.venv/lib/python3.14/site-packages (from pdfplumber) (12.0.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in ./.venv/lib/python3.14/site-packages (from pdfplumber) (5.2.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in ./.venv/lib/python3.14/site-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in ./.venv/lib/python3.14/site-packages (from pdfminer.six==20251107->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in ./.venv/lib/python3.14/site-packages (from beautifulsoup4) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./.venv/lib/python3.14/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.14/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.14/site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.14/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: cffi>=2.0.0 in ./.venv/lib/python3.14/site-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.14/site-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber beautifulsoup4 nltk tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc7b358-38e0-4550-b06c-9197ca389f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dhanarjkusuma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/dhanarjkusuma/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pdfplumber\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2327897-e405-4267-bb98-bf117fd3ae88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preprocessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fe1a552-b481-468a-820b-198c9e3cc79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"\\s+\", \" \", text)        # remove extra spaces\n",
    "    text = re.sub(r\"\\n+\", \"\\n\", text)       # normalize newlines\n",
    "    text = re.sub(r\"\\t+\", \" \", text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b1765-169f-4414-a3d8-154dbde158b8",
   "metadata": {},
   "source": [
    "Fungsi untuk extract text dari file .pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9814e7f3-2a32-4f7c-95aa-a5f1a9b66f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return clean_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817056bc-7e10-48d6-b1f3-f75267db9a4b",
   "metadata": {},
   "source": [
    "Fungsi untuk extract text dari file .html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b3fd8d-3fd7-47d2-bf8f-15a51edf5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html(html_path: str) -> str:\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    # remove script & style\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "    return clean_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf08697-f430-4804-bff5-ec25516224c2",
   "metadata": {},
   "source": [
    "Fungsi untuk extract text dari file .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "057a3520-8d6a-434e-ba32-483b24559eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_txt(txt_path: str) -> str:\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return clean_text(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af22e4e-7ae3-447f-b322-6f71ad40d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 6887.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:00<00:00, 43.22it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 189/189 [00:00<00:00, 19236.66it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 141/141 [00:00<00:00, 10996.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:00<00:00, 12297.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 117/117 [00:00<00:00, 14883.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 123/123 [00:00<00:00, 12935.32it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 108/108 [00:00<00:00, 10805.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 152/152 [00:00<00:00, 11160.92it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 138/138 [00:00<00:00, 10796.55it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 114/114 [00:00<00:00, 14612.07it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:00<00:00, 10024.30it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 39/39 [00:00<00:00, 37500.65it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 109/109 [00:00<00:00, 8690.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 11186.71it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 113/113 [00:00<00:00, 11042.79it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 156/156 [00:00<00:00, 10722.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 111/111 [00:00<00:00, 9717.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:00<00:00, 9504.61it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 93/93 [00:00<00:00, 54869.92it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 114/114 [00:00<00:00, 8326.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 103/103 [00:00<00:00, 10222.02it/s]\n"
     ]
    }
   ],
   "source": [
    "RAW_DIR = \"corpus/raw\"\n",
    "OUT_DIR = \"corpus/processed/docs\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "doc_id = 1\n",
    "\n",
    "for root, _, files in os.walk(RAW_DIR):\n",
    "    for file in tqdm(files):\n",
    "        path = os.path.join(root, file)\n",
    "\n",
    "        try:\n",
    "            if file.endswith(\".html\"):\n",
    "                text = extract_text_from_html(path)\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if len(text) < 500:  # skip dokumen terlalu pendek\n",
    "                continue\n",
    "\n",
    "            out_path = os.path.join(OUT_DIR, f\"doc_{doc_id:02d}.txt\")\n",
    "            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "\n",
    "            doc_id += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c4910-0fd9-404a-8b9d-0c3c055db9fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d37dd6-5366-4bf8-a9db-f8fa1430de23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 44 documents\n"
     ]
    }
   ],
   "source": [
    "DOC_DIR = \"corpus/processed/docs\"\n",
    "docs = sorted(os.listdir(DOC_DIR))\n",
    "print(f\"Loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acb3e423-b524-41c3-9117-1c6b3ca39ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_size: int = 150,\n",
    "    overlap: int = 30\n",
    "):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    chunks = []\n",
    "\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + chunk_size\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = \" \".join(chunk_tokens)\n",
    "\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "        start = end - overlap  # overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2808464e-677d-49db-b0da-1eb9dcb2fbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [00:00<00:00, 589.32it/s]\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "chunk_id = 0\n",
    "\n",
    "for doc_id, file in enumerate(tqdm(docs)):\n",
    "    path = os.path.join(DOC_DIR, file)\n",
    "    \n",
    "    if not file.lower().endswith(\".txt\"):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "    \n",
    "        chunks = chunk_text(text)\n",
    "    \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            all_chunks.append({\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"doc_name\": file,\n",
    "                \"chunk_index\": i,\n",
    "                \"text\": chunk\n",
    "            })\n",
    "            chunk_id += 1\n",
    "            \n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"❌ Error encoding di file: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error lain pada {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41b17c7a-2a61-4eee-b91a-f41bc8d22a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 284 chunks\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "OUT_PATH = \"corpus/chunks/chunks.json\"\n",
    "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
    "\n",
    "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(all_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14d3378e-df33-4a46-b25d-bb3750d432c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 0 ---\n",
      "Doc: doc_01.txt\n",
      "18.000 Data Kendaraan Bocor Lewat Aplikasi Dewa Matel , Pengamat : Pelanggaran Luar Biasa Login dengan Google Login dengan Google . Dibuka di tab baru Login dengan Google Login dengan Google . Dibuka di tab baru Otomatis Mode Gelap Mode Terang Login Gabung KOMPAS.com+ Konten yang disimpan Konten yan ...\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Doc: doc_01.txt\n",
      "Aktual Doa dan Niat Jadwal Sholat Tekno Apps & OS Gadget Internet Hardware Business Game Galeri Indeks Tech Innovation Kilas Internet Otomotif News Mobil Motor Sport Feature Niaga Komunitas Otopedia Galeri Indeks Merapah EV Leadership Elektrifikasi Pameran Bola Timnas Indonesia Liga Indonesia Liga I ...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Doc: doc_01.txt\n",
      "Industri , Inovasi & Infrastruktur Berkurangnya Kesenjangan Kota & Pemukiman yang Berkelanjutan Konsumsi & Produksi yang bertanggungjawab PROGRAM LESTARI Lihat semua Health Penyakit A-Z Kilas Kesehatan Money Ekbis Keuangan Syariah Industri Energi Karier Cuan Belanja Tanya Pajak Indeks Kilas Badan Ki ...\n"
     ]
    }
   ],
   "source": [
    "for c in all_chunks[:3]:\n",
    "    print(\"\\n--- Chunk\", c[\"chunk_id\"], \"---\")\n",
    "    print(\"Doc:\", c[\"doc_name\"])\n",
    "    print(c[\"text\"][:300], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c46fa43-7ac0-4cdd-932c-2dd1a934c2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk stats:\n",
      "  Total chunks      : 284\n",
      "  Avg tokens/chunk  : 133\n",
      "  Min tokens/chunk  : 6\n",
      "  Max tokens/chunk  : 152\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(nltk.word_tokenize(c[\"text\"])) for c in all_chunks]\n",
    "\n",
    "print(\"Chunk stats:\")\n",
    "print(f\"  Total chunks      : {len(all_chunks)}\")\n",
    "print(f\"  Avg tokens/chunk  : {sum(lengths)//len(lengths)}\")\n",
    "print(f\"  Min tokens/chunk  : {min(lengths)}\")\n",
    "print(f\"  Max tokens/chunk  : {max(lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8395e3d-e3a8-4be2-9253-93f3a9bdadc5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf1289-5e00-4843-813d-686679574528",
   "metadata": {},
   "source": [
    "### Load Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b78c8ea8-c6ea-4afe-a035-9350471ac379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"corpus/chunks/chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "texts = [c[\"text\"] for c in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd699378-7076-499c-ac80-c1fc393dbbd3",
   "metadata": {},
   "source": [
    "### Define Compared Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b9ce6a3-3951-41e0-9368-1477a28dd499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.14/site-packages (5.2.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in ./.venv/lib/python3.14/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.14/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.14/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.14/site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.14/site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.venv/lib/python3.14/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./.venv/lib/python3.14/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.14/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (3.20.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.14/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.14/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.14/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.14/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.14/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.14/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.14/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.14/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.14/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.14/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.14/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.14/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.14/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.14/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.14/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.14/site-packages (from requests->transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.3.0 in ./.venv/lib/python3.14/site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./.venv/lib/python3.14/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84d493f2-4723-4e2b-a12e-514bde8dcf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhanarjkusuma/Projects/magister/mini-rag/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "embedding_models = {\n",
    "    \"bge_m3\": \"BAAI/bge-m3\",\n",
    "    \"e5_multilingual\": \"intfloat/multilingual-e5-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27c0944f-4f69-48ae-a8e6-b5951b8e7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding with BAAI/bge-m3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:19<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved corpus/chunks/embeddings_bge_m3.npy | shape = (284, 1024)\n",
      "Embedding with intfloat/multilingual-e5-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:06<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved corpus/chunks/embeddings_e5_multilingual.npy | shape = (284, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "os.makedirs(\"corpus/chunks\", exist_ok=True)\n",
    "\n",
    "for key, model_name in embedding_models.items():\n",
    "    print(f\"Embedding with {model_name}\")\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    out_path = f\"corpus/chunks/embeddings_{key}.npy\"\n",
    "    np.save(out_path, embeddings)\n",
    "\n",
    "    print(f\"Saved {out_path} | shape = {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef332be6-9bf0-45b1-a0d8-711ad7c87d77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Vector Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14f3d7e-4d83-47a5-86a9-4f6e6c072e3d",
   "metadata": {},
   "source": [
    "### Import & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9382d625-0eac-4111-8667-14a83cbbf4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93d706c2-6855-42e8-a413-a1c5be872bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 284\n",
      "Embeddings shape: (284, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Load chunks\n",
    "with open(\"corpus/chunks/chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "\n",
    "# Load embeddings (pilih salah satu / bandingkan dua)\n",
    "embeddings_bge = np.load(\"corpus/chunks/embeddings_bge_m3.npy\")\n",
    "embeddings_e5  = np.load(\"corpus/chunks/embeddings_e5_multilingual.npy\")\n",
    "\n",
    "print(\"Chunks:\", len(chunks))\n",
    "print(\"Embeddings shape:\", embeddings_bge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805bb62c-4d3a-4822-b30b-fc29c87a0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "query_model_bge = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "query_model_e5  = SentenceTransformer(\"intfloat/multilingual-e5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3514e9d5-a1e8-4b32-b4ca-92aa5626401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(\n",
    "    query: str,\n",
    "    model: SentenceTransformer,\n",
    "    embeddings: np.ndarray,\n",
    "    chunks: list,\n",
    "    k: int = 3\n",
    "):\n",
    "    # Encode query\n",
    "    query_emb = model.encode(\n",
    "        [query],\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # Cosine similarity\n",
    "    scores = cosine_similarity(query_emb, embeddings)[0]\n",
    "\n",
    "    # Top-k index\n",
    "    top_k_idx = np.argsort(scores)[::-1][:k]\n",
    "\n",
    "    results = []\n",
    "    for idx in top_k_idx:\n",
    "        results.append({\n",
    "            \"score\": float(scores[idx]),\n",
    "            \"text\": chunks[idx][\"text\"],\n",
    "            \"doc_name\": chunks[idx][\"doc_name\"],\n",
    "            \"chunk_id\": chunks[idx][\"chunk_id\"]\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c0bd98-a14a-44e5-a405-0854d6727e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Compare Embedding Models (k=3) ===\n",
      "\n",
      "--- BGE-M3 ---\n",
      "0.6713 | Aturan ADVERTISEMENT SCROLL TO CONTINUE WITH CONTENT Kepala Balai Besar Meteorologi , Klimatologi , dan Geofisika Wilayah II Banten , Hartanto , mengatakan langit berwarna merah merupakan fenomena opt...\n",
      "0.6474 | terjadi pada Kamis ( 18/12 ) petang . Sejumlah warga mengabadikan momen langka tersebut dan mengunggahnya ke media sosial . Tak sedikit dari mereka yang kebingungan atas peristiwa itu . Apa penyebab l...\n",
      "0.6309 | udara , '' katanya . Hartanto mengimbau masyarakat tetap tenang dan tidak terprovokasi spekulasi liar . Dia meminta agar terus memantau informasi perkembangan cuaca . `` Masyarakat diimbau untuk tetap...\n",
      "\n",
      "--- E5 Multilingual ---\n",
      "0.8535 | Aturan ADVERTISEMENT SCROLL TO CONTINUE WITH CONTENT Kepala Balai Besar Meteorologi , Klimatologi , dan Geofisika Wilayah II Banten , Hartanto , mengatakan langit berwarna merah merupakan fenomena opt...\n",
      "0.8492 | Viral Langit Merah Bikin Geger Pandeglang , Ini Kata BMKG login register Nasional Peristiwa Viral Langit Merah Bikin Geger Pandeglang , Ini Kata BMKG CNN Indonesia Sabtu , 20 Des 2025 18:15 WIB Bagika...\n",
      "0.8468 | udara , '' katanya . Hartanto mengimbau masyarakat tetap tenang dan tidak terprovokasi spekulasi liar . Dia meminta agar terus memantau informasi perkembangan cuaca . `` Masyarakat diimbau untuk tetap...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Compare Embedding Models (k=3) ===\")\n",
    "\n",
    "query = \"Apakah fenomena langit merah itu berbahaya?\"\n",
    "\n",
    "results_bge = retrieve_top_k(\n",
    "    query,\n",
    "    query_model_bge,\n",
    "    embeddings_bge,\n",
    "    chunks,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "results_e5 = retrieve_top_k(\n",
    "    query,\n",
    "    query_model_e5,\n",
    "    embeddings_e5,\n",
    "    chunks,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(\"\\n--- BGE-M3 ---\")\n",
    "for r in results_bge:\n",
    "    print(f\"{r['score']:.4f} | {r['text'][:200]}...\")\n",
    "\n",
    "print(\"\\n--- E5 Multilingual ---\")\n",
    "for r in results_e5:\n",
    "    print(f\"{r['score']:.4f} | {r['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682435b8-0209-4960-a293-fb9c63156406",
   "metadata": {},
   "source": [
    "### Build Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b607aa3c-5de4-4927-ba9e-2bbbaf628091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(retrieved_chunks):\n",
    "    return \"\\n\\n\".join(\n",
    "        [f\"- {c['text']}\" for c in retrieved_chunks]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c985916d-5332-46ef-98dc-8fa9355660a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Aturan ADVERTISEMENT SCROLL TO CONTINUE WITH CONTENT Kepala Balai Besar Meteorologi , Klimatologi , dan Geofisika Wilayah II Banten , Hartanto , mengatakan langit berwarna merah merupakan fenomena optik atmosfer atau hamburan rayleigh . Dia mengatakan peristiwa itu alami atau biasa . Dia menjelaskan fenomena alam ini bisa terjadi karena pembiasan cahaya matahari saat berada di posisi rendah atau menjelang terbenam . Menurutnya , cahaya matahari harus menempuh jarak yang lebih jauh melalui atmosfer bumi untuk sampai ke mata . `` Sehingga hanya warna dengan gelombang panjang seperti merah dan jingga yang mampu menembus atmosfer dan tertangkap oleh mata kita , '' katanya , Jumat ( 19/12 ) . Sebagai informasi , fenomena alam langit merah di wilayah Pandeglang selatan terjadi pada Kamis ( 18/12 ) petang . Sejumlah warga mengabadikan momen langka tersebut dan mengunggahnya ke media sosial . Tak sedikit dari mereka yang kebingungan atas peristiwa itu .\n",
      "\n",
      "- terjadi pada Kamis ( 18/12 ) petang\n"
     ]
    }
   ],
   "source": [
    "context = build_context(results_bge)\n",
    "print(context[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246b2a5-8140-44f4-b183-31db6676d535",
   "metadata": {},
   "source": [
    "## RAG Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fece3c-04e9-4013-9dd0-2b4769ca07c5",
   "metadata": {},
   "source": [
    "Download dependencies google llm and init API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1af0469-c802-4186-b28d-2a038d861cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-genai in ./.venv/lib/python3.14/site-packages (1.56.0)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in ./.venv/lib/python3.14/site-packages (from google-genai) (4.12.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.45.0 in ./.venv/lib/python3.14/site-packages (from google-auth[requests]<3.0.0,>=2.45.0->google-genai) (2.45.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in ./.venv/lib/python3.14/site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in ./.venv/lib/python3.14/site-packages (from google-genai) (2.12.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in ./.venv/lib/python3.14/site-packages (from google-genai) (2.32.5)\n",
      "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in ./.venv/lib/python3.14/site-packages (from google-genai) (9.1.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in ./.venv/lib/python3.14/site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in ./.venv/lib/python3.14/site-packages (from google-genai) (4.15.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.14/site-packages (from google-genai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.14/site-packages (from google-genai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.14/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in ./.venv/lib/python3.14/site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (6.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.venv/lib/python3.14/site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./.venv/lib/python3.14/site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.14/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.14/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.14/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.14/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.14/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.14/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.14/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.14/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.6.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./.venv/lib/python3.14/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (0.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d20d21c-22ca-4856-9b0a-a9dd5d049a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyB6j_sL8ouV29JJNygOZSV2L87aNs6w0Ig\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eec0e11-8654-4228-b6d0-7e20fb0bdbfa",
   "metadata": {},
   "source": [
    "Init LLM Model Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a87f04-a1b7-47c9-8fd6-bcd1f5bee876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "ai_client = genai.Client(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba4b04d1-af15-43c7-a2b2-bf79dd445a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FLASH = \"gemini-2.5-flash\"\n",
    "MODEL_PRO   = \"gemini-2.0-pro\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7876621d-9819-4600-9e4b-74a7fcbe8782",
   "metadata": {},
   "source": [
    "Add generic func to answer question using context or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8846cd6a-45fb-4237-b30e-c52c72eea369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_gemini(\n",
    "    client,\n",
    "    question: str,\n",
    "    context: str = None,\n",
    "    model=MODEL_FLASH\n",
    "):\n",
    "    if context:\n",
    "        prompt = f\"\"\"\n",
    "You are an AI assistant. Answer the question using ONLY the context below.\n",
    "If the answer is not in the context, say \"Information not found in the provided documents.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer concisely:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8db9593-b388-484d-8566-8ec86080d86a",
   "metadata": {},
   "source": [
    "Fill to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d89046d2-a892-4c4c-999e-d98784d63439",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Apakah ada fenomena langit merah di Indonesia akhir-akhir ini?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752ef86-598f-481b-b803-5a8e10de4d75",
   "metadata": {},
   "source": [
    "### Answer with NON RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29e3f078-6fe6-46ed-9d2e-d9fe6b3c7e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NON-RAG ANSWER ===\n",
      "Tidak ada fenomena \"langit merah\" yang tidak biasa atau signifikan yang dilaporkan secara luas di Indonesia akhir-akhir ini.\n",
      "\n",
      "Langit kemerahan saat matahari terbit atau terbenam adalah hal yang normal karena pembiasan cahaya matahari oleh atmosfer. Fenomena langit merah yang mencolok dan tidak biasa biasanya terkait dengan partikel di atmosfer seperti abu vulkanik atau asap dari kebakaran besar, yang tidak terjadi dalam skala besar di seluruh Indonesia akhir-akhir ini.\n"
     ]
    }
   ],
   "source": [
    "answer_non_rag = generate_answer_gemini(ai_client, query, None, MODEL_FLASH)\n",
    "print(\"=== NON-RAG ANSWER ===\")\n",
    "print(answer_non_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b8c22-ddf5-4732-a734-d444c062fd01",
   "metadata": {},
   "source": [
    "### Answer with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca92d0c7-9e29-4e40-8188-2b823aa6fb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RAG ANSWER ===\n",
      "Ya, fenomena langit merah terjadi di wilayah Pandeglang selatan, Indonesia, pada Kamis (18/12) petang.\n"
     ]
    }
   ],
   "source": [
    "# Ambil top-3 chunk (contoh pakai BGE-M3)\n",
    "retrieved_chunks = retrieve_top_k(\n",
    "    query=query,\n",
    "    model=query_model_bge,\n",
    "    embeddings=embeddings_bge,\n",
    "    chunks=chunks,\n",
    "    k=3\n",
    ")\n",
    "\n",
    "context = build_context(retrieved_chunks)\n",
    "\n",
    "answer_rag = generate_answer_gemini(\n",
    "    client=ai_client,\n",
    "    question=query,\n",
    "    context=context,\n",
    "    model=MODEL_FLASH\n",
    ")\n",
    "\n",
    "print(\"=== RAG ANSWER ===\")\n",
    "print(answer_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa27fbd-986d-41bd-bbd1-d2c36ec8b2cc",
   "metadata": {},
   "source": [
    "## Evaluation Using MMR Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93f0bf8d-6aa9-45b9-a4e8-b86015e5bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def mmr(\n",
    "    query_emb,\n",
    "    doc_embs,\n",
    "    docs,\n",
    "    k=3,\n",
    "    lambda_param=0.7\n",
    "):\n",
    "    sim_to_query = cosine_similarity(query_emb, doc_embs)[0]\n",
    "\n",
    "    selected = []\n",
    "    selected_idx = []\n",
    "\n",
    "    for _ in range(k):\n",
    "        scores = []\n",
    "\n",
    "        for i in range(len(docs)):\n",
    "            if i in selected_idx:\n",
    "                scores.append(-np.inf)\n",
    "                continue\n",
    "\n",
    "            diversity = 0\n",
    "            if selected_idx:\n",
    "                diversity = max(\n",
    "                    cosine_similarity(\n",
    "                        doc_embs[i].reshape(1, -1),\n",
    "                        doc_embs[selected_idx]\n",
    "                    )[0]\n",
    "                )\n",
    "\n",
    "            score = (\n",
    "                lambda_param * sim_to_query[i]\n",
    "                - (1 - lambda_param) * diversity\n",
    "            )\n",
    "            scores.append(score)\n",
    "\n",
    "        idx = int(np.argmax(scores))\n",
    "        selected_idx.append(idx)\n",
    "        selected.append(docs[idx])\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc40e1fd-b5b8-45fe-9242-418228912e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in ./.venv/lib/python3.14/site-packages (6.2.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in ./.venv/lib/python3.14/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in ./.venv/lib/python3.14/site-packages (from gradio) (4.12.0)\n",
      "Requirement already satisfied: audioop-lts<1.0 in ./.venv/lib/python3.14/site-packages (from gradio) (0.2.2)\n",
      "Requirement already satisfied: brotli>=1.1.0 in ./.venv/lib/python3.14/site-packages (from gradio) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in ./.venv/lib/python3.14/site-packages (from gradio) (0.127.0)\n",
      "Requirement already satisfied: ffmpy in ./.venv/lib/python3.14/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.2 in ./.venv/lib/python3.14/site-packages (from gradio) (2.0.2)\n",
      "Requirement already satisfied: groovy~=0.1 in ./.venv/lib/python3.14/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in ./.venv/lib/python3.14/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in ./.venv/lib/python3.14/site-packages (from gradio) (0.36.0)\n",
      "Requirement already satisfied: jinja2<4.0 in ./.venv/lib/python3.14/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in ./.venv/lib/python3.14/site-packages (from gradio) (3.0.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in ./.venv/lib/python3.14/site-packages (from gradio) (2.4.0)\n",
      "Requirement already satisfied: orjson~=3.0 in ./.venv/lib/python3.14/site-packages (from gradio) (3.11.5)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.14/site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in ./.venv/lib/python3.14/site-packages (from gradio) (2.3.3)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in ./.venv/lib/python3.14/site-packages (from gradio) (12.0.0)\n",
      "Requirement already satisfied: pydantic<=3.0,>=2.0 in ./.venv/lib/python3.14/site-packages (from gradio) (2.12.5)\n",
      "Requirement already satisfied: pydub in ./.venv/lib/python3.14/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in ./.venv/lib/python3.14/site-packages (from gradio) (0.0.21)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in ./.venv/lib/python3.14/site-packages (from gradio) (6.0.3)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in ./.venv/lib/python3.14/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in ./.venv/lib/python3.14/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in ./.venv/lib/python3.14/site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in ./.venv/lib/python3.14/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in ./.venv/lib/python3.14/site-packages (from gradio) (0.20.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./.venv/lib/python3.14/site-packages (from gradio) (4.15.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in ./.venv/lib/python3.14/site-packages (from gradio) (0.40.0)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.14/site-packages (from gradio-client==2.0.2->gradio) (2025.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.14/site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in ./.venv/lib/python3.14/site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.14/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.14/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.14/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.14/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.14/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.14/site-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.14/site-packages (from pydantic<=3.0,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./.venv/lib/python3.14/site-packages (from pydantic<=3.0,>=2.0->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./.venv/lib/python3.14/site-packages (from pydantic<=3.0,>=2.0->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.14/site-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.14/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.14/site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.14/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.14/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.14/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.14/site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.14/site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e77e572-d200-40f0-9dc2-2ccf87a34ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "\n",
    "def answer_with_k(query, k):\n",
    "    answers_dict = []\n",
    "\n",
    "    # Non RAG\n",
    "    answer_non_rag = generate_answer_gemini(\n",
    "        ai_client, \n",
    "        query, \n",
    "        None, \n",
    "        MODEL_FLASH\n",
    "    )\n",
    "    answers_dict.append({\n",
    "        'k': -1,\n",
    "        'embeddings': 'Non-RAG',\n",
    "        'answers': answer_non_rag,\n",
    "    })\n",
    "\n",
    "    \n",
    "    # embeddings: bge-m3\n",
    "    retrieved = retrieve_top_k(\n",
    "        query=query,\n",
    "        model=query_model_bge,\n",
    "        embeddings=embeddings_bge,\n",
    "        chunks=chunks,\n",
    "        k=k\n",
    "    )\n",
    "    context = build_context(retrieved)\n",
    "    answer = generate_answer_gemini(\n",
    "        client=ai_client,\n",
    "        question=query,\n",
    "        context=context,\n",
    "        model=MODEL_FLASH\n",
    "    )\n",
    "    answers_dict.append({\n",
    "        'k': k,\n",
    "        'embeddings': 'BAAI/bge-m3',\n",
    "        'answers': answer,\n",
    "    })\n",
    "\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # embeddings: e5\n",
    "    retrieved = retrieve_top_k(\n",
    "        query=query,\n",
    "        model=query_model_e5,\n",
    "        embeddings=embeddings_e5,\n",
    "        chunks=chunks,\n",
    "        k=k\n",
    "    )\n",
    "    context = build_context(retrieved)\n",
    "    answer = generate_answer_gemini(\n",
    "        client=ai_client,\n",
    "        question=query,\n",
    "        context=context,\n",
    "        model=MODEL_FLASH\n",
    "    )\n",
    "    answers_dict.append({\n",
    "        'k': k,\n",
    "        'embeddings': 'intfloat/multilingual-e5-base',\n",
    "        'answers': answer,\n",
    "    })\n",
    "    return answers_dict\n",
    "\n",
    "\n",
    "def rag_chat(question):\n",
    "    answers = []\n",
    "\n",
    "    answers_k = answer_with_k(query=question, k=1)\n",
    "    answers.extend(answers_k)\n",
    "    \n",
    "    \n",
    "    answers_k = answer_with_k(query=question, k=2)\n",
    "    answers.extend(answers_k)\n",
    "    \n",
    "    answers_k = answer_with_k(query=question, k=3)\n",
    "    answers.extend(answers_k)\n",
    "\n",
    "    final_answers = \"\"\n",
    "    for ans in answers:\n",
    "        final_answers += \"\\n\"\n",
    "        answers_text = f\"\"\"\n",
    "            k={ans['k']}\n",
    "            embeddings={ans['embeddings']}\n",
    "            answers={ans['answers']}\n",
    "        \"\"\"\n",
    "        final_answers += answers_text\n",
    "    \n",
    "    return final_answers\n",
    "\n",
    "#print(rag_chat(\"Apakah ada fenomena langit merah di Indonesia akhir-akhir ini?\"))\n",
    "\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=rag_chat,\n",
    "    inputs=gr.Textbox(label=\"Ask a question\"),\n",
    "    outputs=gr.Textbox(label=\"RAG Answer\"),\n",
    "    title=\"Mini RAG Demo\",\n",
    "    description=\"Simple Retrieval-Augmented Generation system\"\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef593a-083d-43a9-8ddb-7912246b3481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Mini-RAG)",
   "language": "python",
   "name": "mini-rag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
